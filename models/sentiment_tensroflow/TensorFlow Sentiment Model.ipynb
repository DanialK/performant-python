{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, History, CSVLogger\n",
    "import operator\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import feather\n",
    "ps = PorterStemmer()\n",
    "# import keras.backend as K\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = '../../data/embeddings'\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_balanced = feather.read_dataframe(\"../../assets/balanced_reviews.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = joblib.load('../../assets/tokenizer.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore('../../assets/yelp_x_y_test_train.h5') as h:\n",
    "    X_train = h['X_train'].values\n",
    "    X_test = h['X_test'].values\n",
    "    y_train = h['y_train'].values\n",
    "    y_test = h['y_test'].values\n",
    "WORD_INDEX_SORTED = sorted(tokenizer.word_index.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, './glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(WORD_INDEX_SORTED))\n",
    "embedding_matrix = np.zeros((MAX_NUM_WORDS, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = LSTM(\n",
    "    64,\n",
    "    kernel_initializer='glorot_normal',\n",
    "    recurrent_initializer='glorot_normal'\n",
    ")(embedded_sequences)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96308 samples, validate on 24077 samples\n",
      "Epoch 1/20\n",
      "96308/96308 [==============================] - 92s 959us/sample - loss: 0.4972 - accuracy: 0.7571 - val_loss: 0.4421 - val_accuracy: 0.7964\n",
      "Epoch 2/20\n",
      "96308/96308 [==============================] - 90s 937us/sample - loss: 0.3930 - accuracy: 0.8234 - val_loss: 0.3905 - val_accuracy: 0.8192\n",
      "Epoch 3/20\n",
      "96308/96308 [==============================] - 90s 935us/sample - loss: 0.3599 - accuracy: 0.8400 - val_loss: 0.3498 - val_accuracy: 0.8443\n",
      "Epoch 4/20\n",
      "96308/96308 [==============================] - 90s 935us/sample - loss: 0.3350 - accuracy: 0.8517 - val_loss: 0.3454 - val_accuracy: 0.8456\n",
      "Epoch 5/20\n",
      "96308/96308 [==============================] - 90s 937us/sample - loss: 0.3226 - accuracy: 0.8576 - val_loss: 0.3232 - val_accuracy: 0.8598\n",
      "Epoch 6/20\n",
      "96308/96308 [==============================] - 90s 932us/sample - loss: 0.3120 - accuracy: 0.8639 - val_loss: 0.3467 - val_accuracy: 0.8451\n",
      "Epoch 7/20\n",
      "96308/96308 [==============================] - 90s 932us/sample - loss: 0.3021 - accuracy: 0.8677 - val_loss: 0.3108 - val_accuracy: 0.8660\n",
      "Epoch 8/20\n",
      "96308/96308 [==============================] - 90s 931us/sample - loss: 0.2950 - accuracy: 0.8710 - val_loss: 0.3198 - val_accuracy: 0.8609\n",
      "Epoch 9/20\n",
      "96308/96308 [==============================] - 90s 930us/sample - loss: 0.2897 - accuracy: 0.8742 - val_loss: 0.3033 - val_accuracy: 0.8690\n",
      "Epoch 10/20\n",
      "96308/96308 [==============================] - 90s 930us/sample - loss: 0.2820 - accuracy: 0.8780 - val_loss: 0.2975 - val_accuracy: 0.8707\n",
      "Epoch 11/20\n",
      "96308/96308 [==============================] - 90s 930us/sample - loss: 0.2748 - accuracy: 0.8813 - val_loss: 0.2992 - val_accuracy: 0.8712\n",
      "Epoch 12/20\n",
      "96308/96308 [==============================] - 90s 931us/sample - loss: 0.2699 - accuracy: 0.8841 - val_loss: 0.3246 - val_accuracy: 0.8586\n",
      "Epoch 13/20\n",
      "96308/96308 [==============================] - 90s 931us/sample - loss: 0.2688 - accuracy: 0.8842 - val_loss: 0.2943 - val_accuracy: 0.8741\n",
      "Epoch 14/20\n",
      "96308/96308 [==============================] - 90s 930us/sample - loss: 0.2631 - accuracy: 0.8865 - val_loss: 0.3035 - val_accuracy: 0.8725\n",
      "Epoch 15/20\n",
      "96308/96308 [==============================] - 90s 930us/sample - loss: 0.2580 - accuracy: 0.8890 - val_loss: 0.2846 - val_accuracy: 0.8782\n",
      "Epoch 16/20\n",
      "96308/96308 [==============================] - 89s 929us/sample - loss: 0.2546 - accuracy: 0.8910 - val_loss: 0.2996 - val_accuracy: 0.8751\n",
      "Epoch 17/20\n",
      "96308/96308 [==============================] - 90s 930us/sample - loss: 0.2490 - accuracy: 0.8938 - val_loss: 0.2849 - val_accuracy: 0.8785\n",
      "Epoch 18/20\n",
      "96308/96308 [==============================] - 90s 931us/sample - loss: 0.2480 - accuracy: 0.8948 - val_loss: 0.3127 - val_accuracy: 0.8690\n",
      "Epoch 19/20\n",
      "96308/96308 [==============================] - 90s 930us/sample - loss: 0.2420 - accuracy: 0.8970 - val_loss: 0.2850 - val_accuracy: 0.8797\n",
      "Epoch 20/20\n",
      "96308/96308 [==============================] - 89s 929us/sample - loss: 0.2378 - accuracy: 0.8989 - val_loss: 0.2844 - val_accuracy: 0.8820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f26203d01d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=512,\n",
    "          epochs=20,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../../assets/sentiment_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"../../assets/sentiment_tensorflow/model_weights.h5\")\n",
    "model.save(\"../../assets/sentiment_tensorflow/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
