{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, History, CSVLogger\n",
    "import operator\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import feather\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = '../../data/embeddings'\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, './glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minority_balance_dataframe_by_multiple_categorical_variables(df, categorical_columns=None, downsample_by=0.1):\n",
    "    \"\"\"\n",
    "    :param df: pandas.DataFrame\n",
    "    :param categorical_columns: iterable of categorical columns names contained in {df}\n",
    "    :return: balanced pandas.DataFrame\n",
    "    \"\"\"\n",
    "    if categorical_columns is None or not all([c in df.columns for c in categorical_columns]):\n",
    "        raise ValueError('Please provide one or more columns containing categorical variables')\n",
    "\n",
    "    minority_class_combination_count = df.groupby(categorical_columns).apply(lambda x: x.shape[0]).min()\n",
    "    \n",
    "    minority_class_combination_count = int(minority_class_combination_count * downsample_by)\n",
    "    \n",
    "    df = df.groupby(categorical_columns).apply(\n",
    "        lambda x: x.sample(minority_class_combination_count)\n",
    "    ).drop(categorical_columns, axis=1).reset_index().set_index('level_1')\n",
    "\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reviews = ''\n",
    "\n",
    "with open('../../data/yelp-dataset/review.json','r') as f:\n",
    "    for line in f.readlines()[0:3000000]:\n",
    "        reviews += line\n",
    "\n",
    "df_reviews = pd.read_json(reviews, lines=True)\n",
    "del reviews\n",
    "# df_reviews =  pd.read_json('../../data/yelp-dataset/review.json', lines=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_reviews['len'] = df_reviews.text.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_reviews = df_reviews[df_reviews['len'].between(10, 4000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balancing dataset\n",
    "df_rev_balanced = minority_balance_dataframe_by_multiple_categorical_variables(\n",
    "    df_reviews, print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, './glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "    categorical_columns=['stars'], \n",
    "    downsample_by=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_balanced.reset_index().to_feather('../../data/balanced_reviews.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ../../assets/balanced_reviews.feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../../assets/sentiment_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(df_rev_balanced.text.tolist())\n",
    "joblib.dump(tokenizer, '../../assets/sentiment_tensorflow/tokenizer.pickle')\n",
    "\n",
    "WORD_INDEX_SORTED = sorted(tokenizer.word_index.items(), key=operator.itemgetter(1))\n",
    "\n",
    "seqs = tokenizer.texts_to_sequences(df_rev_balanced.text.values)\n",
    "X = pad_sequences(seqs, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "Y = df_rev_balanced.stars.values.astype(int)\n",
    "Y_cat = [1 if y > 3 else 0 for y in Y]\n",
    "assert X.shape[0] == Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_cat, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore('../../data/yelp_x_y_test_train.h5') as h:\n",
    "    h['X_train'] = pd.DataFrame(X_train)\n",
    "    h['X_test'] = pd.DataFrame(X_test)\n",
    "    h['y_train'] = pd.DataFrame(y_train)\n",
    "    h['y_test'] = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "    \"MAX_SEQUENCE_LENGTH\": MAX_SEQUENCE_LENGTH\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../assets/sentiment_tensorflow/configuration.json', 'w') as f:\n",
    "    json.dump(configuration, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
